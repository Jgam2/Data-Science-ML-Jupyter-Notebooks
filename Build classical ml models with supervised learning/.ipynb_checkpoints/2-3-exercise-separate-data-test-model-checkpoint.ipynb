{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7141b0df-dd02-4176-b307-1bebd4489a02",
   "metadata": {},
   "source": [
    "# Exercise: Supervised learning\n",
    "\n",
    "Recall our farming scenario, in which we want to look at how January temperatures have changed over time. Now we'll build a model that achieves this by using supervised learning. \n",
    "\n",
    "With many libraries, we can build a model in only a few lines of code. Here, we'll break down the process into steps so that we can explore how things work.\n",
    "\n",
    "## Four components\n",
    "Recall that there are four key components to supervised learning: the data, the model, the cost function, and the optimizer. Let's inspect these one at a time.\n",
    "\n",
    "### 1. The data\n",
    "\n",
    "We'll use publicly available weather data for Seattle. Let's load that and restrict it to January temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9ec3ef-5c46-422f-af95-af834965113d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'seattleWeather_1948-2017.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c88bcbe54be5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m# Load a file that contains weather data for Seattle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'seattleWeather_1948-2017.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'date'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;31m# Keep only January temperatures\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    608\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    460\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    461\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 462\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    463\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    464\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    817\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1048\u001b[0m             )\n\u001b[0;32m   1049\u001b[0m         \u001b[1;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1050\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[call-arg]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1051\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1052\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   1865\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1866\u001b[0m         \u001b[1;31m# open handles\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1867\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1868\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1869\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"storage_options\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"encoding\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"memory_map\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_open_handles\u001b[1;34m(self, src, kwds)\u001b[0m\n\u001b[0;32m   1360\u001b[0m         \u001b[0mLet\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mreaders\u001b[0m \u001b[0mopen\u001b[0m \u001b[0mIOHanldes\u001b[0m \u001b[0mafter\u001b[0m \u001b[0mthey\u001b[0m \u001b[0mare\u001b[0m \u001b[0mdone\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mtheir\u001b[0m \u001b[0mpotential\u001b[0m \u001b[0mraises\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m         \"\"\"\n\u001b[1;32m-> 1362\u001b[1;33m         self.handles = get_handle(\n\u001b[0m\u001b[0;32m   1363\u001b[0m             \u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1364\u001b[0m             \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    640\u001b[0m                 \u001b[0merrors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 642\u001b[1;33m             handle = open(\n\u001b[0m\u001b[0;32m    643\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    644\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'seattleWeather_1948-2017.csv'"
     ]
    }
   ],
   "source": [
    "import pandas\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/m0b_optimizer.py\n",
    "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/seattleWeather_1948-2017.csv\n",
    "\n",
    "# Load a file that contains weather data for Seattle\n",
    "data = pandas.read_csv('seattleWeather_1948-2017.csv', parse_dates=['date'])\n",
    "\n",
    "# Keep only January temperatures\n",
    "data = data[[d.month == 1 for d in data.date]].copy()\n",
    "\n",
    "\n",
    "# Print the first and last few rows\n",
    "# Remember that with Jupyter notebooks, the last line of \n",
    "# code is automatically printed\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0b0b5f-08de-413a-8ccf-117583f922f7",
   "metadata": {},
   "source": [
    "We have data from 1948 to 2017, split across 2,170 rows. \n",
    "\n",
    "We'll analyze the relationship between `date` and daily minimum temperatures. Let's take a quick look at our data as a graph. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472992f-be99-4bcd-89a9-a4f0afcd787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import graphing # Custom graphing code. See our GitHub repository for details\n",
    "\n",
    "# Let's take a quick look at our data\n",
    "graphing.scatter_2D(data, label_x=\"date\", label_y=\"min_temperature\", title=\"January Temperatures (°F)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac6965e-193c-4909-a5ac-23f540362434",
   "metadata": {},
   "source": [
    "Machine learning usually works best when the X and Y axes have roughly the same range of values. We'll cover why in later learning material. For now, let's just scale our data slightly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2426d0-7340-4db0-a8d7-5db7aab33662",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This block of code scales and offsets the data slightly, which helps the training process\n",
    "# You don't need to understand this code. We'll cover these concepts in later learning material\n",
    "\n",
    "# Offset date into number of years since 1982\n",
    "data[\"years_since_1982\"] = [(d.year + d.timetuple().tm_yday / 365.25) - 1982 for d in data.date]\n",
    "\n",
    "# Scale and offset temperature so that it has a smaller range of values\n",
    "data[\"normalised_temperature\"] = (data[\"min_temperature\"] - np.mean(data[\"min_temperature\"])) / np.std(data[\"min_temperature\"])\n",
    "\n",
    "# Graph\n",
    "graphing.scatter_2D(data, label_x=\"years_since_1982\", label_y=\"normalised_temperature\", title=\"January Temperatures (Normalised)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e82b2a27-69f1-43d4-bfc5-8c38e81c64c2",
   "metadata": {},
   "source": [
    "### 2. The model\n",
    "\n",
    "We'll select a simple linear regression model. This model uses a line to make estimates. You might have come across trendlines like these before when making graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7ec0ef-112a-42d9-953f-b322339f7e10",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel:\n",
    "\n",
    "    def __init__(self):\n",
    "        '''\n",
    "        Creates a new MyModel\n",
    "        '''\n",
    "        # Straight lines described by two parameters:\n",
    "        # The slope is the angle of the line\n",
    "        self.slope = 0\n",
    "        # The intercept moves the line up or down\n",
    "        self.intercept = 0\n",
    "\n",
    "    def predict(self, date):\n",
    "        '''\n",
    "        Estimates the temperature from the date\n",
    "        '''\n",
    "        return date * self.slope + self.intercept\n",
    "\n",
    "# Create our model ready to be trained\n",
    "model = MyModel()\n",
    "\n",
    "print(\"Model made!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376103da-afad-4987-a737-6d94f9048450",
   "metadata": {},
   "source": [
    "We wouldn't normally use a model before it has been trained, but for the sake of learning, let's take a quick look at it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9fe8b4-5ebf-4c8f-9f7b-35835b9eca85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Model parameters before training: {model.intercept}, {model.slope}\")\n",
    "\n",
    "# Look at how well the model does before training\n",
    "print(\"Model visualised before training:\")\n",
    "graphing.scatter_2D(data, \"years_since_1982\", \"normalised_temperature\", trendline=model.predict)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22674a12-d714-423f-a1cc-db23ff80e2e9",
   "metadata": {},
   "source": [
    "You can see that before training, our model (the red line) isn't useful at all. It always simply predicts zero.\n",
    "\n",
    "### 3. The cost (objective) function\n",
    "\n",
    "Our next step is to create a _cost function_ (_objective function_).\n",
    "\n",
    "These functions in supervised learning compare the model's estimate to the correct answer. In our case, our label is temperature, so our cost function will compare the estimated temperature to temperatures seen in the historical records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "398f22d5-23d4-49fc-b40b-237e42a731b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_function(actual_temperatures, estimated_temperatures):\n",
    "    '''\n",
    "    Calculates the difference between actual and estimated temperatures\n",
    "    Returns the difference, and also returns the squared difference (the cost)\n",
    "\n",
    "    actual_temperatures: One or more temperatures recorded in the past\n",
    "    estimated_temperatures: Corresponding temperature(s) estimated by the model\n",
    "    '''\n",
    "\n",
    "    # Calculate the difference between actual temperatures and those\n",
    "    # estimated by the model\n",
    "    difference = estimated_temperatures - actual_temperatures\n",
    "\n",
    "    # Convert to a single number that tells us how well the model did\n",
    "    # (smaller numbers are better)\n",
    "    cost = sum(difference ** 2)\n",
    "\n",
    "    return difference, cost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94559f3-5a83-4343-a50f-a28d2cc66b3c",
   "metadata": {},
   "source": [
    "### 4. The optimizer\n",
    "\n",
    "The role of the optimizer is to guess new parameter values for the model. \n",
    "\n",
    "We haven't covered optimizers in detail yet, so to make things simple, we'll use an prewritten optimizer. You don't need to understand how this works, but if you're curious, you can find it in our GitHub repository. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a136eee-4252-4a98-807d-125a54c3f585",
   "metadata": {},
   "outputs": [],
   "source": [
    "from m0b_optimizer import MyOptimizer\n",
    "\n",
    "# Create an optimizer\n",
    "optimizer = MyOptimizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1222aaf2-9703-4e98-ba88-fd7c8442b2d8",
   "metadata": {},
   "source": [
    "## The training loop\n",
    "\n",
    "Let's put these components together so that they train the model. \n",
    "\n",
    "First, let's make a function that performs one iteration of training. Read each step carefully in the following code. If you want, add some `print()` statements inside the method to help you see the training in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "709c96cb-3c39-4355-ab59-1bbd6dc42d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_iteration(model_inputs, true_temperatures, last_cost:float):\n",
    "    '''\n",
    "    Runs a single iteration of training.\n",
    "\n",
    "\n",
    "    model_inputs: One or more dates to provide the model (dates)\n",
    "    true_temperatues: Corresponding temperatures known to occur on those dates\n",
    "\n",
    "    Returns:\n",
    "        A Boolean, as to whether training should continue\n",
    "        The cost calculated (small numbers are better)\n",
    "    '''\n",
    "\n",
    "    # === USE THE MODEL ===\n",
    "    # Estimate temperatures for all data that we have\n",
    "    estimated_temperatures = model.predict(model_inputs)\n",
    "\n",
    "    # === OBJECTIVE FUNCTION ===\n",
    "    # Calculate how well the model is working\n",
    "    # Smaller numbers are better \n",
    "    difference, cost = cost_function(true_temperatures, estimated_temperatures)\n",
    "\n",
    "    # Decide whether to keep training\n",
    "    # We'll stop if the training is no longer improving the model effectively\n",
    "    if cost >= last_cost:\n",
    "        # Stop training\n",
    "        return False, cost\n",
    "    else:\n",
    "        # === OPTIMIZER ===\n",
    "        # Calculate updates to parameters\n",
    "        intercept_update, slope_update = optimizer.get_parameter_updates(model_inputs, cost, difference)\n",
    "\n",
    "        # Change the model parameters\n",
    "        model.slope += slope_update\n",
    "        model.intercept += intercept_update\n",
    "\n",
    "        return True, cost\n",
    "\n",
    "print(\"Training method ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e22625-f821-422a-9d61-8aa557a8153f",
   "metadata": {},
   "source": [
    "Let's run a few iterations manually, so that you can watch how training works.\n",
    "\n",
    "Run the following code several times, and note how the model changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1265ded3-cf9a-4ecf-b6c2-d7ab64bc28cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "print(f\"Model parameters before training:\\t\\t{model.intercept:.8f},\\t{model.slope:.8f}\")\n",
    "\n",
    "continue_loop, cost = train_one_iteration(model_inputs = data[\"years_since_1982\"],\n",
    "                                                    true_temperatures = data[\"normalised_temperature\"],\n",
    "                                                    last_cost = math.inf)\n",
    "\n",
    "print(f\"Model parameters after 1 iteration of training:\\t{model.intercept:.8f},\\t{model.slope:.8f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6552d86-2494-4c98-9a96-3bed5d5c739a",
   "metadata": {},
   "source": [
    "It will take thousands of iterations to train the model well, so let's wrap it in a loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ac2cfcc-b843-4516-ae4d-ea4294f83255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Start the loop\n",
    "print(\"Training beginning...\")\n",
    "last_cost = math.inf\n",
    "i = 0\n",
    "continue_loop = True\n",
    "while continue_loop:\n",
    "\n",
    "    # Run one iteration of training\n",
    "    # This will tell us whether to stop training, and also what\n",
    "    # the cost was for this iteration\n",
    "    continue_loop, last_cost = train_one_iteration(model_inputs = data[\"years_since_1982\"],\n",
    "                                                    true_temperatures = data[\"normalised_temperature\"],\n",
    "                                                    last_cost = last_cost)\n",
    "   \n",
    "    # Print the status\n",
    "    if i % 400 == 0:\n",
    "        print(\"Iteration:\", i)\n",
    "\n",
    "    i += 1\n",
    "\n",
    "    \n",
    "print(\"Training complete!\")\n",
    "print(f\"Model parameters after training:\\t{model.intercept:.8f},\\t{model.slope:.8f}\")\n",
    "graphing.scatter_2D(data, \"years_since_1982\", \"normalised_temperature\", trendline=model.predict)    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c175937-4222-4bfc-b975-0455989650eb",
   "metadata": {},
   "source": [
    "Notice how now the model is trained. It's giving more sensible predictions about January temperatures.\n",
    "\n",
    "Interestingly, the model shows temperatures going up over time. Perhaps we need to stop feeding grain to our elk earlier in the year!\n",
    "\n",
    "## Summary\n",
    "\n",
    "In this exercise, we split up supervised learning into its individual stages to see what's going on in code when we use third-party libraries. The important point to take away is how these pieces fit together. Note that most parts of this process require data."
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "conda-env-py37_default-py"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
