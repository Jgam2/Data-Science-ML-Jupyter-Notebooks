{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Build a confusion matrix\n",
        "\n",
        "\n",
        "In this exercise we go into more detail on measuring the perfomance of classification models, using the concepts of *imbalanced datasets*, *accuracy* and *confusion matrices*.\n",
        "\n",
        "\n",
        "## Data visualization\n",
        "\n",
        "Our new dataset represents different classes of objects found in snow.\n",
        "\n",
        "Let's start this exercise by loading in and having a look at our data:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\n",
        "\n",
        "#Import the data from the .csv file\n",
        "dataset = pandas.read_csv('snow_objects.csv', delimiter=\"\\t\")\n",
        "\n",
        "#Let's have a look at the data\n",
        "dataset"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Exploration\n",
        "\n",
        "We can see that the dataset has both continuous (`size`, `roughness`, `motion`) and categorical data (`color` and `label`).\n",
        "Let's do some quick data exploration and see what different label classes we have and their respective counts:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import graphing # custom graphing code. See our GitHub repo for details\n",
        "\n",
        "# Plot a histogram with counts for each label\n",
        "graphing.multiple_histogram(dataset, label_x=\"label\", label_group=\"label\", title=\"Label distribution\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The histogram above makes it very easy to understand both the labels we have in the dataset and their distribution.\n",
        "\n",
        "One important information to notice is that this is an *imbalanced dataset*: classes are not represented in the same proportion (we have 4 times more rocks and trees than animals, for example).\n",
        "\n",
        "This is relevant as imbalanced sets are very common \"in the wild\", and in the future we will learn how to address that to build better models.\n",
        "\n",
        "We can do the same analisys for the `color` feature:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot a histogram with counts for each label\n",
        "graphing.multiple_histogram(dataset, label_x=\"color\", label_group=\"color\", title=\"Color distribution\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can notice that:\n",
        "\n",
        "- We have `8` different color categories.\n",
        "- The `color` feature is also heavily imbalanced.\n",
        "- Out plotting algorithm is not smart enough to assign the correct colors to their respective names.\n",
        "\n",
        "Let's see what we can find about the other features:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "graphing.box_and_whisker(dataset, label_y=\"size\", title='Boxplot of \"size\" feature')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Above we can see that the majority of our samples are relatively small, with sizes ranging from `0` to `70`, but we have a few much bigger outliers.\n",
        "\n",
        "Let's take a look at the `roughness` feature:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "graphing.box_and_whisker(dataset, label_y=\"roughness\", title='Boxplot of \"roughness\" feature')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's not a lot of variation here: values for `roughness` range from `0` to a little over `2`, with most samples having values close to the mean.\n",
        "\n",
        "Finally, let's plot the `motion` feature:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "graphing.box_and_whisker(dataset, label_y=\"motion\", title='Boxplot of \"motion\" feature')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most objects seem to be either static or moving very slowly. There is a smaller number of objects moving faster, with a couple of outliers over `10`."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the data above one could assume that the smaller and faster objects are likely hikers and animals, whereas the bigger, more static elements are trees and rocks.\n",
        "\n",
        "## Building a classification model\n",
        "\n",
        "Let's build and train a classification model using a random forest, to predict the class of an object based on the features in our dataset:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split the dataset in an 70/30 train/test ratio. \n",
        "train, test = train_test_split(dataset, test_size=0.3, random_state=2)\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train our model, using the `train` dataset we've just created:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the model\n",
        "model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False)\n",
        "\n",
        "# Define which features are to be used (leave color out for now)\n",
        "features = [\"size\", \"roughness\", \"motion\"]\n",
        "\n",
        "# Train the model\n",
        "model.fit(train[features], train.label)\n",
        "\n",
        "print(\"Model trained!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assessing our model\n",
        "\n",
        "We can now use our newly trained model to make predictions using the *test* set.\n",
        "\n",
        "By comparing the values predicted to the actual labels (also called *true* values), we can measure the model's performance using different *metrics*.\n",
        "\n",
        "*Accuracy*, for example, is the simply number of correctly predicted labels out of all predictions performed:\n",
        "\n",
        "```sh\n",
        "    Accuracy = Correct Predictions / Total Predictions\n",
        "```\n",
        "\n",
        "Let's see how this can be done in code:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Import a function that measures a models accuracy\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Calculate the model's accuracy on the TEST set\n",
        "actual = test.label\n",
        "predictions = model.predict(test[features])\n",
        "\n",
        "# Return accuracy as a fraction\n",
        "acc = accuracy_score(actual, predictions)\n",
        "\n",
        "# Return accuracy as a number of correct predictions\n",
        "acc_norm = accuracy_score(actual, predictions, normalize=False)\n",
        "\n",
        "print(f\"The random forest model's accuracy on the test set is {acc:.4f}.\")\n",
        "print(f\"It correctly predicted {acc_norm} labels in {len(test.label)} predictions.\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our model __seems__ to be doing quite well!\n",
        "\n",
        "That intuition, however, can be misleading:\n",
        "\n",
        "- Accuracy does not take into account the __wrong__ predictions made by the model\n",
        "\n",
        "- It's also not very good at painting a clear picture in *class-imbalanced datasets*, like ours, where the number of possible classes is not evenly distributed (recall that we have 800 trees, 800 rocks, but only 200 animals)"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Building a confusion matrix\n",
        "\n",
        "A *confusion matrix* is a table where we compare the actual labels to what the model predicted. It gives us a more detailed understanding of how the model is doing and where it's getting thigs right or missing.\n",
        "\n",
        "This is one of the ways we can do that in code:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn has a very convenient utility to build confusion matrices\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Build and print our confusion matrix, using the actual values and predictions \n",
        "# from the test set, calculated in previous cells\n",
        "cm = confusion_matrix(actual, predictions, normalize=None)\n",
        "\n",
        "print(\"Confusion matrix for the test set:\")\n",
        "print(cm)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the matrix above can be useful in calculations, it's not very helpful or intuitive.\n",
        "\n",
        "Let's add a plot with labels and colors to turn that data into actual insights:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# We use plotly to create plots and charts\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Create the list of unique labels in the test set, to use in our plot\n",
        "# I.e., ['animal', 'hiker', 'rock', 'tree']\n",
        "x = y = sorted(list(test[\"label\"].unique()))\n",
        "\n",
        "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
        "fig = ff.create_annotated_heatmap(cm, x, y)\n",
        "\n",
        "# Set titles and ordering\n",
        "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
        "                    yaxis = dict(categoryorder = \"category descending\"))\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=0.5,\n",
        "                        y=-0.15,\n",
        "                        showarrow=False,\n",
        "                        text=\"Predicted label\",\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=-0.15,\n",
        "                        y=0.5,\n",
        "                        showarrow=False,\n",
        "                        text=\"Actual label\",\n",
        "                        textangle=-90,\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "# We need margins so the titles fit\n",
        "fig.update_layout(margin=dict(t=80, r=20, l=100, b=50))\n",
        "fig['data'][0]['showscale'] = True\n",
        "fig.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the plot has the __Actual labels__ on the `y-axis` and __Predicted labels__ on the `x-axis`, as defined by the `confusion_matrix` function call.\n",
        "\n",
        "Let's go over the cells in the __top row__, left to right, to understand what the plot is telling us:\n",
        "\n",
        "- The first cell means we had `28` animals (the real value) and our model predicted we had `28` animals (the prediction). Those are all TPs or *true positives*.\n",
        "\n",
        "__However__,\n",
        "\n",
        "- The second cell indicates the model __incorrectly__ classified `38` animals as hikers (thus `38` FNs or *false negatives* in that cell). That doesn't look good!\n",
        "- Third and fourth cells are correct: they each have `0` animals classified as rocks or trees (thus `0` *false negatives*). \n",
        "\n",
        "Now, onto the __second row__ from the top:\n",
        "\n",
        "- First cell has `30` hikers classified as animals; In this case we have `30` FPs or *false positives* in that cell.\n",
        "- In the second cell we have `103` hikers correctly classified (TPs or *true positives*)\n",
        "- In the third cell we have `1` hiker classified as a rock! (`1` FN)\n",
        "- Finally, in the fourth cell we have `0` hikers classified as trees (meaning `0` FNs)\n",
        "\n",
        "If we go over the same process for the remaining rows, we can see that the model is generally accurate, but only because we have so many rocks and trees in our set and because it does well on those classes.\n",
        "\n",
        "When it comes to hikers and animals the model gets confused (it shows a high number of FPs and FNs), but because these classes are less represented in the dataset the *accuracy score* remains high.\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "In this exercise we talked about the following concepts:\n",
        "\n",
        "- *Imbalanced datasets*, where features or classes can be represented by a disproportionate number of samples.\n",
        "- *Accuracy* as a metric to assess model performance, and its shortcomings.\n",
        "- How to generate, plot and interpret *confusion matrices*, to get a better understanding of how a classification model is performing."
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "conda-env-py37_default-py",
      "language": "python",
      "display_name": "py37_default"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-py37_default-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}