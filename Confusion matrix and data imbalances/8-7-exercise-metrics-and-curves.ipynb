{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: More metrics derived from confusion matrices\n",
        "\n",
        "In this exercise we will learn about different metrics, using them to explain the results obtained from the *binary classification model* we built in the previous unit.\n",
        "\n",
        "## Data visualization\n",
        "\n",
        "We will use the dataset with different classes of objects found on the mountain one more time:\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "import numpy\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\n",
        "\n",
        "#Import the data from the .csv file\n",
        "dataset = pandas.read_csv('snow_objects.csv', delimiter=\"\\t\")\n",
        "\n",
        "#Let's have a look at the data\n",
        "dataset"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that to use the dataset above for *binary classification*, we need to add another column to the dataset, and set it to `True` where the original label is `hiker`, and `False` where it's not.\n",
        "\n",
        "Let's then add that label, split the dataset and train the model again:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Add a new label with true/false values to our dataset\n",
        "dataset[\"is_hiker\"] = dataset.label == \"hiker\"\n",
        "\n",
        "# Split the dataset in an 70/30 train/test ratio. \n",
        "train, test = train_test_split(dataset, test_size=0.3, random_state=1, shuffle=True)\n",
        "\n",
        "# define a random forest model\n",
        "model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False)\n",
        "\n",
        "# Define which features are to be used \n",
        "features = [\"size\", \"roughness\", \"motion\"]\n",
        "\n",
        "# Train the model using the binary label\n",
        "model.fit(train[features], train.is_hiker)\n",
        "\n",
        "print(\"Model trained!\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now use this model to predict whether objects in the snow are hikers or not.\n",
        "\n",
        "Let's plot its *confusion matrix*:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn has a very convenient utility to build confusion matrices\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Calculate the model's accuracy on the TEST set\n",
        "actual = test.is_hiker\n",
        "predictions = model.predict(test[features])\n",
        "\n",
        "# Build and print our confusion matrix, using the actual values and predictions \n",
        "# from the test set, calculated in previous cells\n",
        "cm = confusion_matrix(actual, predictions, normalize=None)\n",
        "\n",
        "# Create the list of unique labels in the test set, to use in our plot\n",
        "# I.e., ['True', 'False',]\n",
        "unique_targets = sorted(list(test[\"is_hiker\"].unique()))\n",
        "\n",
        "# Convert values to lower case so the plot code can count the outcomes\n",
        "x = y = [str(s).lower() for s in unique_targets]\n",
        "\n",
        "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
        "fig = ff.create_annotated_heatmap(cm, x, y)\n",
        "\n",
        "# Set titles and ordering\n",
        "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
        "                    yaxis = dict(categoryorder = \"category descending\"))\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=0.5,\n",
        "                        y=-0.15,\n",
        "                        showarrow=False,\n",
        "                        text=\"Predicted label\",\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=-0.15,\n",
        "                        y=0.5,\n",
        "                        showarrow=False,\n",
        "                        text=\"Actual label\",\n",
        "                        textangle=-90,\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "# We need margins so the titles fit\n",
        "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
        "fig['data'][0]['showscale'] = True\n",
        "fig.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's also calculate some values that will be used throughout this exercise\n",
        "# We already have actual values and corresponding predictions, defined above\n",
        "correct = actual == predictions\n",
        "tp = numpy.sum(correct & actual)\n",
        "tn = numpy.sum(correct & numpy.logical_not(actual))\n",
        "fp = numpy.sum(numpy.logical_not(correct) & actual)\n",
        "fn = numpy.sum(numpy.logical_not(correct) & numpy.logical_not(actual))\n",
        "\n",
        "print(\"TP - True Positives: \", tp)\n",
        "print(\"TN - True Negatives: \", tn)\n",
        "print(\"FP - False positives: \", fp)\n",
        "print(\"FN - False negatives: \", fn)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the values and matrix above to help us understand other metrics.\n",
        "\n",
        "\n",
        "## Calculating metrics\n",
        "\n",
        "From here on we will take a closer look at each at the following metrics, how they are calculated and how they can help explain our current model. \n",
        "\n",
        "* Accuracy\n",
        "* Sensitivity/Recall\n",
        "* Specificity\n",
        "* Precision\n",
        "* False positive rate\n",
        "\n",
        "Let's first recall some useful terms:\n",
        "\n",
        "* TP = True positives: a positive label is correctly predicted\n",
        "* TN = True nositives: a negative label is correctly predicted\n",
        "* FP = False positives: a negative label is predicted as a positive\n",
        "* FN = False negatives: a positive label is predicted as a negative\n",
        "\n",
        "\n",
        "### Accuracy\n",
        "Accuracy is the number of correct predictions divided by the total number of predictions:\n",
        "\n",
        "```\n",
        "    accuracy = (TP+TN) / number of samples\n",
        "```\n",
        "\n",
        "It's possibly the most basic metric used but, as we've seen, it's not the most reliable when *imbalanced datasets* are used.\n",
        "\n",
        "In code:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate accuracy\n",
        "# len(actual) is the number of samples in the set that generated TP and TN\n",
        "accuracy = (tp+tn) / len(actual) \n",
        "\n",
        "# print result as a percentage\n",
        "print(f\"Model accuracy is {accuracy:.2f}%\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sensitivity/Recall\n",
        "\n",
        "*Sensitivity* and *Recall* are interchangeable names for the same metric, which expresses the fraction of samples __correctly__ predicted by a model:\n",
        "\n",
        "\n",
        "```\n",
        "    sensitivity = recall = TP / (TP + FN)\n",
        "```\n",
        "\n",
        "This is an important metric, that tells us how out of all the existing __positive__ samples, how many are __correctly__ predicted.\n",
        "\n",
        "In code:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# code for sensitivity/recall\n",
        "sensitivity = recall = tp / (tp + fn)\n",
        "\n",
        "# print result as a percentage\n",
        "print(f\"Model sensitivity/recall is {sensitivity:.2f}%\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Specificity\n",
        "Specificity expresses the fraction of __negative__ labels correctly predicted over the total number of existing negative samples:\n",
        "\n",
        "```\n",
        "    specificity = TN / (TN + FP)\n",
        "```\n",
        "\n",
        "It can be calculated using the following code:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for specificity\n",
        "specificity = tn / (tn + fp)\n",
        "\n",
        "# print result as a percentage\n",
        "print(f\"Model specificity is {specificity:.2f}%\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Precision\n",
        "Precision expresses the proportion of __correctly__ predicited positive samples over all positive predictions:\n",
        "\n",
        "```\n",
        "    precision = TP / (TP + FP)\n",
        "```\n",
        "In other words, it indicates how out of all positive predictions, how many are trully positive labels.\n",
        "\n",
        "It can be calculated using the following code:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for precision\n",
        "\n",
        "precision = tp / (tp + fp)\n",
        "\n",
        "# print result as a percentage\n",
        "print(f\"Model precision is {precision:.2f}%\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "### False positive rate\n",
        "False positive rate or FPR, is the number of __incorrect__ positive predictions divided by the total number of negative samples:\n",
        "\n",
        "```\n",
        "    false_positive_rate = FP / (FP + TN)\n",
        "```\n",
        "\n",
        "\n",
        "In code:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for false positive rate\n",
        "false_positive_rate = fp / (fp + tn)\n",
        "\n",
        "# print result as a percentage\n",
        "print(f\"Model false positive rate is {false_positive_rate:.2f}%\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the sum of `specificity` and `false positive rate` should always be equal to `1`.\n",
        "\n",
        "## Conclusion\n",
        "\n",
        "There are several different metrics that can help us evaluate the performance of a model, in the context of the quality of its predictions.\n",
        "\n",
        "The choice of the most adequate metrics, however, is primarily a funciton of the data and the problem we are trying to solve."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "We covered the following topics in this unit:\n",
        "\n",
        "* How to calculate the very basic measurements used in the evaluation of classification models: TP, FP, TN, FN.\n",
        "* How to use the measurement aboves to calculate more meaningful metrics, such as:\n",
        "    * Accuracy\n",
        "    * Sensitivity/Recall\n",
        "    * Specificity\n",
        "    * Precision\n",
        "    * False positive rate\n",
        "* How the choice of metrics depends on the dataset and the problem we are trying to solve.\n",
        "\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "conda-env-py37_default-py",
      "language": "python",
      "display_name": "py37_default"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-py37_default-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}