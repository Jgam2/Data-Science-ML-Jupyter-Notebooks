{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Exercise: Imbalanced data model bias\n",
        "\n",
        "In this exercise, we will take a closer look at *imbalanced datasets*, what effects they have on predictions and how they can be addressed.\n",
        "\n",
        "We will also employ *confusion matrices* to evaluate model updates.\n",
        "\n",
        "## Data visualization\n",
        "\n",
        "Just like in the previous exercise, we use a dataset that represents different classes of objects found on the mountain:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects.csv\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/snow_objects_balanced.csv\n",
        "\n",
        "#Import the data from the .csv file\n",
        "dataset = pandas.read_csv('snow_objects.csv', delimiter=\"\\t\")\n",
        "\n",
        "# Let's have a look at the data\n",
        "dataset"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "scrolled": true
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that we have an *imbalanced dataset*. Some classes are much more frequent than others:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import graphing # custom graphing code. See our GitHub repo for details\n",
        "\n",
        "# Plot a histogram with counts for each label\n",
        "graphing.multiple_histogram(dataset, label_x=\"label\", label_group=\"label\", title=\"Label distribution\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using binary classification\n",
        "\n",
        "For this exercise we will build a *binary classification model*. We want to predict if objects in the snow are \"hikers\" or \"not-hikers\".\n",
        "\n",
        "To do that, we first need to add another column to our dataset, and set it to `True` where the original label is `hiker`, and `False` to anything else:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Add a new label with true/false values to our dataset\n",
        "dataset[\"is_hiker\"] = dataset.label == \"hiker\"\n",
        "\n",
        "# Plot frequency for new label\n",
        "graphing.multiple_histogram(dataset, label_x=\"is_hiker\", label_group=\"is_hiker\", title=\"Distribution for new binary label 'is_hiker'\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have only two classes of labels in our dataset, but we have made it even more imbalanced.\n",
        "\n",
        "Let's train the random forest model using `is_hiker` as the target variable, then measure its accuracy on both *train* and *test* sets:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "# import matplotlib.pyplot as plt\n",
        "# from sklearn.metrics import plot_confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Custom function that measures accuracy on different models and datasets\n",
        "# We will use this in different parts of the exercise\n",
        "def assess_accuracy(model, dataset, label):\n",
        "    \"\"\"\n",
        "    Asesses model accuracy on different sets\n",
        "    \"\"\" \n",
        "    actual = dataset[label]        \n",
        "    predictions = model.predict(dataset[features])\n",
        "    acc = accuracy_score(actual, predictions)\n",
        "    return acc\n",
        "\n",
        "# Split the dataset in an 70/30 train/test ratio. \n",
        "train, test = train_test_split(dataset, test_size=0.3, random_state=1, shuffle=True)\n",
        "\n",
        "# define a random forest model\n",
        "model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False)\n",
        "\n",
        "# Define which features are to be used (leave color out for now)\n",
        "features = [\"size\", \"roughness\", \"motion\"]\n",
        "\n",
        "# Train the model using the binary label\n",
        "model.fit(train[features], train.is_hiker)\n",
        "\n",
        "print(\"Train accuracy:\", assess_accuracy(model,train, \"is_hiker\"))\n",
        "print(\"Test accuracy:\", assess_accuracy(model,test, \"is_hiker\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy looks good for both *train* and *test* sets, but remember that this metric is not an absolute measure of success.\n",
        "\n",
        "We should plot a confusion matrix to see how the model is actually doing:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# sklearn has a very convenient utility to build confusion matrices\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import plotly.figure_factory as ff\n",
        "\n",
        "# Calculate the model's accuracy on the TEST set\n",
        "actual = test.is_hiker\n",
        "predictions = model.predict(test[features])\n",
        "\n",
        "# Build and print our confusion matrix, using the actual values and predictions \n",
        "# from the test set, calculated in previous cells\n",
        "cm = confusion_matrix(actual, predictions, normalize=None)\n",
        "\n",
        "# Create the list of unique labels in the test set, to use in our plot\n",
        "# I.e., ['True', 'False',]\n",
        "unique_targets = sorted(list(test[\"is_hiker\"].unique()))\n",
        "\n",
        "# Convert values to lower case so the plot code can count the outcomes\n",
        "x = y = [str(s).lower() for s in unique_targets]\n",
        "\n",
        "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
        "fig = ff.create_annotated_heatmap(cm, x, y)\n",
        "\n",
        "# Set titles and ordering\n",
        "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
        "                    yaxis = dict(categoryorder = \"category descending\")\n",
        "                    )\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=0.5,\n",
        "                        y=-0.15,\n",
        "                        showarrow=False,\n",
        "                        text=\"Predicted label\",\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=-0.15,\n",
        "                        y=0.5,\n",
        "                        showarrow=False,\n",
        "                        text=\"Actual label\",\n",
        "                        textangle=-90,\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "# We need margins so the titles fit\n",
        "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
        "fig['data'][0]['showscale'] = True\n",
        "fig.show()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix shows us that, despite the reported metrics, the model is not incredibly precise.\n",
        "\n",
        "Out of the 660 samples present in the *test* set (30% of the total samples), it predicted `29` *false negatives* and `33` _false positives_.\n",
        "\n",
        "More importantly, look at the bottom row, which shows what happened when the model was shown information about a hiker: it got the answer wrong almost 30% of the time. This means it would not correctly identify almost 30% of the people on the mountain!  \n",
        "\n",
        "What happens if we used this model to make predictions on balanced sets?\n",
        "\n",
        "Let's load a dataset with an equal number of outcomes for \"hikers\" and \"non-hikers\", then use that data to make predictions:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Load and print umbiased set\n",
        "#Import the data from the .csv file\n",
        "balanced_dataset = pandas.read_csv('snow_objects_balanced.csv', delimiter=\"\\t\")\n",
        "\n",
        "#Let's have a look at the data\n",
        "graphing.multiple_histogram(balanced_dataset, label_x=\"label\", label_group=\"label\", title=\"Label distribution\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This new dataset is balanced among the classes, but for our purposes we want it balanced between hikers and non-hikers. \n",
        "\n",
        "For simplicity, let's take the hikers plus a random sampling of the non-hikers."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Add a new label with true/false values to our dataset\n",
        "balanced_dataset[\"is_hiker\"] = balanced_dataset.label == \"hiker\"\n",
        "\n",
        "hikers_dataset = balanced_dataset[balanced_dataset[\"is_hiker\"] == 1] \n",
        "nonhikers_dataset = balanced_dataset[balanced_dataset[\"is_hiker\"] == False] \n",
        "# take a random sampling of non-hikers the same size as the hikers subset\n",
        "nonhikers_dataset = nonhikers_dataset.sample(n=len(hikers_dataset.index), random_state=1)\n",
        "balanced_dataset = pandas.concat([hikers_dataset, nonhikers_dataset])\n",
        "\n",
        "# Plot frequency for \"is_hiker\" labels\n",
        "graphing.multiple_histogram(balanced_dataset, label_x=\"is_hiker\", label_group=\"is_hiker\", title=\"Label distribution in balanced dataset\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, the `is_hiker` label has the same number of `True` and `False` for both classes. We are now using a *class balanced dataset*.\n",
        "\n",
        "Let's run predictions on this set using the previously trained model:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model using a balanced dataset\n",
        "actual = balanced_dataset.is_hiker\n",
        "predictions = model.predict(balanced_dataset[features])\n",
        "\n",
        "# Build and print our confusion matrix, using the actual values and predictions \n",
        "# from the test set, calculated in previous cells\n",
        "cm = confusion_matrix(actual, predictions, normalize=None)\n",
        "\n",
        "# Print accuracy using this set\n",
        "print(\"Balanced set accuracy:\", assess_accuracy(model,balanced_dataset, \"is_hiker\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, we see a noticeable drop in accuracy using a different set.\n",
        "\n",
        "Again, let's visually analyze its performance:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# plot new confusion matrix\n",
        "# Create the list of unique labels in the test set to use in our plot\n",
        "unique_targets = sorted(list(balanced_dataset[\"is_hiker\"].unique()))\n",
        "\n",
        "# Convert values to lower case so the plot code can count the outcomes\n",
        "x = y = [str(s).lower() for s in unique_targets]\n",
        "\n",
        "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
        "fig = ff.create_annotated_heatmap(cm, x, y)\n",
        "\n",
        "# Set titles and ordering\n",
        "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
        "                    yaxis = dict(categoryorder = \"category descending\")\n",
        "                    )\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=0.5,\n",
        "                        y=-0.15,\n",
        "                        showarrow=False,\n",
        "                        text=\"Predicted label\",\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=-0.15,\n",
        "                        y=0.5,\n",
        "                        showarrow=False,\n",
        "                        text=\"Actual label\",\n",
        "                        textangle=-90,\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "# We need margins so the titles fit\n",
        "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
        "fig['data'][0]['showscale'] = True\n",
        "fig.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix confirms the poor accuracy using this dataset, but why is this happening when we had such excellent metrics in the earlier *train* and *test* sets?\n",
        "\n",
        "Recall that the first model was heavily imbalanced. The \"hiker\" class made up roughly 22% of the outcomes.\n",
        "\n",
        "When such an imbalance happens, classification models don't have enough data to learn the patterns for the minority __class__, and as a consequence become biased towards the __majority__ class!\n",
        "\n",
        "Imbalanced sets can be addressed in a number of ways:\n",
        "\n",
        "- Improving data selection\n",
        "- Resampling the dataset\n",
        "- Using weighted classes\n",
        "\n",
        "For this exercise, we will focus on the last option."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Using class weights to balance dataset\n",
        "\n",
        "We can assign different *weights* to the majority and minority classes, according to their distribution, and modify our training algorithm so that it takes that information into account during the training phase.\n",
        "\n",
        "It will then penalize errors when the minority class is misclassified, in essence \"forcing\" the model to to better learn their features and patterns.\n",
        "\n",
        "To use weighted classes, we have to retrain our model using the original *train* set, but this time telling the algorithm to use weights when calculating errors:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Import function used in calculating weights\n",
        "from sklearn.utils import class_weight\n",
        "\n",
        "# Retrain model using class weights\n",
        "# Using class_weight=\"balanced\" tells the algorithm to automatically calculate weights for us\n",
        "weighted_model = RandomForestClassifier(n_estimators=1, random_state=1, verbose=False, class_weight=\"balanced\")\n",
        "# Train the weighted_model using binary label\n",
        "weighted_model.fit(train[features], train.is_hiker)\n",
        "\n",
        "print(\"Train accuracy:\", assess_accuracy(weighted_model,train, \"is_hiker\"))\n",
        "print(\"Test accuracy:\", assess_accuracy(weighted_model, test, \"is_hiker\"))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "After using the weighted classes, the *train* accuracy remained almost the same, while the *test* accuracy showed a small improvement (roughly 1%).\n",
        "\n",
        "Let's see if results are improved at all using the __balanced__ set for predictions again:\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Balanced set accuracy:\", assess_accuracy(weighted_model, balanced_dataset, \"is_hiker\"))\n",
        "\n",
        "# Test the weighted_model using a balanced dataset\n",
        "actual = balanced_dataset.is_hiker\n",
        "predictions = weighted_model.predict(balanced_dataset[features])\n",
        "\n",
        "# Build and print our confusion matrix, using the actual values and predictions \n",
        "# from the test set, calculated in previous cells\n",
        "cm = confusion_matrix(actual, predictions, normalize=None)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy for the balanced set increased roughly 4%, but we should still try to to visualize and understand the new results.\n",
        "\n",
        "## Final confusion matrix\n",
        "\n",
        "We can now plot a final confusion matrix, representing predictions for a *balanced dataset*, using a model trained on a *weighted class dataset*:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the matrix above as a heatmap with annotations (values) in its cells\n",
        "fig = ff.create_annotated_heatmap(cm, x, y)\n",
        "\n",
        "# Set titles and ordering\n",
        "fig.update_layout(  title_text=\"<b>Confusion matrix</b>\", \n",
        "                    yaxis = dict(categoryorder = \"category descending\")\n",
        "                    )\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=0.5,\n",
        "                        y=-0.15,\n",
        "                        showarrow=False,\n",
        "                        text=\"Predicted label\",\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "fig.add_annotation(dict(font=dict(color=\"black\",size=14),\n",
        "                        x=-0.15,\n",
        "                        y=0.5,\n",
        "                        showarrow=False,\n",
        "                        text=\"Actual label\",\n",
        "                        textangle=-90,\n",
        "                        xref=\"paper\",\n",
        "                        yref=\"paper\"))\n",
        "\n",
        "# We need margins so the titles fit\n",
        "fig.update_layout(margin=dict(t=80, r=20, l=120, b=50))\n",
        "fig['data'][0]['showscale'] = True\n",
        "fig.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "While the results might look a bit disappointing, we now have 21% wrong predictions (FNs + FPs), against 25% from the previous experiment.\n",
        "\n",
        "Correct predictions (TPs + TNs) went from 74.7% to 78.7%.\n",
        "\n",
        "Is an all around 4% improvement significant or not?\n",
        "\n",
        "Remember that we had relatively little data to train the model, and the features we have available may still be so similar for different samples (for example, hikers and animals tend to be small, non-rough and move a lot), that despite our efforts, the model still has some difficulty making correct predictions.\n",
        "\n",
        "We only had to change a single line of code to get better results, so it seems worth the effort!\n",
        "\n"
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "\n",
        "This was a long exercise, where we covered the following topics:\n",
        "\n",
        "- Creating new label fields so we can perform *binary classification* using a dataset with multiple classes.\n",
        "- How training on *imbalanced sets* can have a negative effect in perfomance, especially when using unseen data from *balanced datasets*.\n",
        "- Evaluating results of *binary classification* models using a confusion matrix.\n",
        "- Using weighted classes to address class imbalances when training a model and evaluating the results.\n",
        "\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "conda-env-py37_default-py",
      "language": "python",
      "display_name": "py37_default"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-py37_default-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}