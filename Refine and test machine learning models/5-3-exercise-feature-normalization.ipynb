{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Data Normalization\n",
        "Normalizing or standardizing are very similar techniques that change the range of values that a feature has. Doing so helps models learn faster and more robustly. \n",
        "\n",
        "Both of these processes are commonly referred to as *feature scaling*.\n",
        "\n",
        "In this exercise we'll use a dog training dataset to predict how many rescues a dog will perform on a given year, based on how old they were when their training began.\n",
        "\n",
        "We will train models with and without feature scaling and compare their behaviour and results.\n",
        "\n",
        "But first, let's load our dataset and inspect it:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/dog-training.csv\n",
        "!wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/m1b_gradient_descent.py\n",
        "data = pandas.read_csv(\"dog-training.csv\", delimiter=\"\\t\")\n",
        "data.head()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset above tells us at what age a dog began training, how many rescues, on average, they have performed per year, and other stats, like what age they were last year, their weight and how many rescues they performed in that period.\n",
        "\n",
        "Note that we also have variables expressed in different units, such as  `month_old_when_trained` in months, `age_last_year` in years, and `weight_last_year` in kilograms.\n",
        "\n",
        "Having features in widely different ranges and units is a good indicator that a model can benefit from feature scaling.\n",
        "\n",
        "First, let's train our model using the dataset \"as is\":"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from m1b_gradient_descent import gradient_descent\n",
        "import numpy\n",
        "import graphing\n",
        "\n",
        "# Train model using Gradient Descent\n",
        "# This method uses custom code that will print out progress as training advances.\n",
        "# You don't need to inspect how this works for these exercises, but if you are\n",
        "# curious, you can find it in out GitHub repository\n",
        "model = gradient_descent(data.month_old_when_trained, data.mean_rescues_per_year, learning_rate=5E-4, number_of_iterations=8000)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "tags": []
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Analysis\n",
        "As you can see in the output above we're printing an estimate of weights and the calculated cost at each iteration.\n",
        "\n",
        "The final line in the output shows that the model stopped training because it reached its maximum allowed number of iterations, but the cost could still be lower if we had let it run longer.\n",
        "\n",
        "Let's plot the model at the end of this training:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the data and trendline after training\n",
        "graphing.scatter_2D(data, \"month_old_when_trained\", \"mean_rescues_per_year\", trendline=model.predict)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot above tells us that the younger a dog begins training, the more rescues it be perform in a year.\n",
        "\n",
        "Notice that it doesn't fit the data very well (most points are above the line). That's due to training being cut off early, before the model could find the optimal weights.\n",
        "\n",
        "\n",
        "## Normalizing data\n",
        "Let's use *normalization* as the form of *feature scaling* for this model, applying it to the `month_old_when_trained` feature:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Add the normalized verions of \"age_when_trained\" to the dataset.\n",
        "# Notice that it \"centers\" the mean age around 0\n",
        "data[\"normalized_age_when_trained\"] = (data.month_old_when_trained - numpy.mean(data.month_old_when_trained)) / (numpy.std(data.month_old_when_trained))\n",
        "\n",
        "# Print a sample of the new dataset\n",
        "data[:5]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice the the values `normalized_age_when_trained` column above are distributed in a much smaller range (between -2 and 2) and have their mean centered around `0`.\n",
        "\n",
        "## Visualizing Scaled Features\n",
        "Let's use a box plot to compare the original feature values to their normalised versions:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "from plotly.subplots import make_subplots\n",
        "import plotly.graph_objects as go\n",
        "import plotly.express as px\n",
        "\n",
        "fig = px.box(data,y=[\"month_old_when_trained\", \"normalized_age_when_trained\"])\n",
        "fig.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now compare the two features by hovering your mouse over the graph. You will see that:\n",
        "\n",
        " - `month_old_when_trained` ranges from 1 to 71 and has its median centered around 35.\n",
        "\n",
        " - `normalized_age_when_trained` ranges from -1.6381 to 1.6798, and is centered exactly at 0.\n",
        "\n",
        "## Training with normalized features\n",
        "We can now retrain our model using the normalized feature in our dataset:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's retrain our model, this time using the normalised feature\n",
        "model_norm = gradient_descent(data.normalized_age_when_trained, data.mean_rescues_per_year, learning_rate=5E-4, number_of_iterations=8000)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Let's take a look at that output again.\n",
        "\n",
        "Despite still being allowed a maximum of 8000 iterations, the model stopped at the 5700 mark.\n",
        "\n",
        "Why? Because this time, using the normalized feature, it was quickly able to reach a point where the cost could no longer be improved.\n",
        "\n",
        "In other words, it \"converged\" much faster than the previous version.\n",
        "\n",
        "## Plotting the normalized model\n",
        "\n",
        "We can now plot the new model and see the results of normalization:"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot the data and trendline again, after training with normalized feature\n",
        "graphing.scatter_2D(data, \"normalized_age_when_trained\", \"mean_rescues_per_year\", trendline=model_norm.predict)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "It looks like this model fits the data much better that the first one!\n",
        "\n",
        "The normalized model shows a larger slope and data now centered on `0` on the X-axis, both factors which should allow the model to converge faster.\n",
        "\n",
        "But how much faster?\n",
        "\n",
        "Let's plot a comparison between models to visualize the improvements."
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "cost1 = model.cost_history\n",
        "cost2 = model_norm.cost_history\n",
        "\n",
        "# Creates dataframes with the cost history for each model\n",
        "df1 = pandas.DataFrame({\"cost\": cost1, \"Model\":\"No feature scaling\"})\n",
        "df1[\"number of iterations\"] = df1.index + 1\n",
        "df2 = pandas.DataFrame({\"cost\": cost2, \"Model\":\"With feature scaling\"})\n",
        "df2[\"number of iterations\"] = df2.index + 1\n",
        "\n",
        "# Concatenate dataframes into a single one that we can use in our plot\n",
        "df = pandas.concat([df1, df2])\n",
        "\n",
        "# Plot cost history for both models\n",
        "fig = graphing.scatter_2D(df, label_x=\"number of iterations\", label_y=\"cost\", title=\"Training Cost vs Iterations\", label_colour=\"Model\")\n",
        "fig.update_traces(mode='lines')\n",
        "fig.show()"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "This plots clearly shows that using a normalized dataset allowed our model to converge much faster. Reaching the lowest cost and finding the optimal weights required a much smaller number of iterations.\n",
        "\n",
        "This is very important when you are developing a new model, as it allows you to iterate quicker, but also when your model is deployed to a production environment, as it will require less compute time for training and costing less than a \"slow\" model.\n",
        "\n",
        "Most modern libraries handle normalization transparently, so this is usually something often don't have to worry about unless implementing something complex.\n",
        "\n",
        "However, when we use *Gradient Descent*, _feature scaling_ can become quite important, especially in more complex models where the features come in different units and ranges."
      ],
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary\n",
        "In this exercise we covered the following concepts:\n",
        "\n",
        "- _Feature scalaing_ techniques are used to improve the efficiency of training models\n",
        "- How to add a normalized feature to a dataset\n",
        "- How to visualize normalized features and compare them to their original values\n",
        "\n",
        "Finally, we compared the performance of models before and after using normalized features, using plots to visualize the improvements\n",
        "\n"
      ],
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "conda-env-py37_default-py",
      "language": "python",
      "display_name": "py37_default"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernel_info": {
      "name": "conda-env-py37_default-py"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}